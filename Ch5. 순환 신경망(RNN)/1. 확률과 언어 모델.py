### 확률과 언어 모델 ###

### word2vec을 확률 관점에서 바라보다
# word2vec의 CBOW에서는 맥락 w.t-1과 w.t+1로 w.t를 추측하는 일을 수행합니다.(window size 1이라 가정)
# 이때 w.t-1과 w.t+1이 주어졌을 때 타깃이 w.t가 될 확률을 수식으로 나타내면 다음과 같다.
# P(w.t | w.t-1,w.t+1) # w.t-1과 w.t+1이 일어났을 때 w.t가 일어날 확률
# 만약 맥락을 양쪽이 아닌 왼쪽으로만 고려한다면 확률을 다음과 같다
# P(w.t | w.t-2,w.t-1)
# 위와 같이 표기한다면 CBOW모델이 다루는 손실함수는 다음과 같이 표기할 수 있다.(교차 엔트로피 오차)
# L = -logP(w.t|w.t-2,w.t-1)
# 모델의 학습은 해당 손실함수의 손실 값을 최소화 하는 파라미터를 찾는 일이다!
# 이처럼 CBOW의 본래 목적은 맥락으로 부터 타깃을 더 정확하게 추측하는 일이다.

# 그렇다면 CBOW의 본래 목적인 "맥락으로부터 타깃을 추측하는 것"은 어디에 이용할 수 있을까?
# P(w.t | w.t-2,w.t-1) 이 식은 어떻게 실용적으로 쓰여져야 할까? ==> '언어 모델' 등장!


# cf) word2vec에서 맥락의 윈도우 크기는 하이퍼파라미터 이다!(임의의 값으로 설정)


# 확률의 곱셈정리
# P(A,B) = P(A|B)P(B) : (A,B 모두 일어날 확률) = (B가 일어날 확률)x(B가 일어났을때 A가 일어날 확률)
# cf) P(A,B) = P(A|B)P(B) = P(B|A)P(A)

### 언어 모델
# 언어 모델은 '단어 나열'에 확률을 부여한다.
# 특정한 단어의 시퀀스에 대해서 그 시퀀스가 일어날 가능성이 어느 정도인지(얼마나 자연스러운지) 확률로 평가!
# ex) "you say goodbye" 라는 단어 시퀀스에는 높은 확률을 출력 (0.092)
# ex) "you say gooddie" 라는 단어 시퀀스에는 낮은 확률을 출력 (0.000000032)

# 언어 모델은 단어 순서의 자연스러움을 확률적으로 평가할 수 있으므로 새로운 문장을 생성하는 용도로도 이용할 수 있다.

# 언어 모델을 수식으로 설명하면 다음과 같다.
# w1, ... w.m 의 단어가 순서로 출현할 확률(P(w1,...w.m)). 여러 사건이 동시에 일어날 확률 이므로 동시 확률이라고 한다.
# 확률의 곱셈정리를 P(w1,...w.m)에 적용하면 언어 모델을 다음과 같이 수식화 할 수 있다.
# P(w1,...w.m) = P(w.m|w1,...w.m-1) x P(w.m-1 |w1,...w.m-2) x ... x P(w3|w2,w1) x P(w2|w1) x P(w1)
# = (t=1~m).π P(w.t|w1,...w.t-1)

# 여기서 주목할 것은 사후확률은 타깃 단어보다 왼쪽에 있는 모든 단어를 맥락(조건)으로 했을 때의 확률이라는 것이다.
# 우리의 목표는 (t=1~m).π P(w.t|w1,...w.t-1)를 구하는 것이고 이를 구한다면 P(w1,...w.m)를 알 수 있다.
# 즉, 단어 순서의 자연스러움을 확률적으로 평가할 수 있다!(각 문장이 올바른 문장인지를 확률적으로 평가할 수 있다!)



### CBOW 모델을 언어 모델로?
# word2vec의 CBOW를 강제로 언어모델에 적용한다면 어떻게 될까? (맥락을 왼쪽 2개로 한정했을 때)
# P(w1,...w.m) = (t=1~m).π P(w.t|w1,...w.t-1) ? (t=1~m).π P(w.t|w.t-2,w.t-1)
# cf) 맥락의 크기는 임의의 결정할 수 있다!
# 더 많은 정보를 고려하기 위해 맥락의 크기를 늘린다고해도 결국 고려하지 못하는 단어가 발생하며, 맥락의 크기를 무한정 늘릴 수도 없다...

# ex) 
# Tom was watching TV in his room. Mary came into the room. Mary said hi to [ ? ]
# 위 빈칸을 추론하려면 ?로부터 18번째나 앞에 나오는 Tom을 기억해야 한다.
# 맥락의 크기를 무한정 늘릴 수 있다 하더라도 CBOW 모델 특성상 맥락안의 단어 순서가 무시된다는 한계가 있다.
# CBOW에서는 맥락을 input으로 넣는데 이 때 각각의 input의 합이 은닉층의 단어 벡터가 되기 때문에 단어의 순서가 무시된다.
# 따로따로 저장하면 되지 않느냐? => 맥락의 크기에 비례해 가중치 매개변수도 늘어나게 된다.(not good)
# ==> RNN 등장!

# RNN은 맥락이 아무리 길더라도 그 맥락의 정보를 기억하는 메커니즘을 갖고 있다.
# 아무리 긴 시계열 데이터에라도 대응할 수 있다!

# cf)
# word2vec은 단어 분산 표현을 얻는 목적으로 고안된 기법이지 이를 언어 모델로 사용하는 경우는 거의 없다.


