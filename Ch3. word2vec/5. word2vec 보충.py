### word2vec 보충 ###

### CBOW 모델과 확률
# cross entrophy의 loss 는 다음과 같다
# L = -k.Σ (t.k * log (y.k))

# CBOW 모델의 추론 결과는 word_to_id의 단어 갯수를 차원을 가지는 벡터이고 각 벡터에는 입력값(맥락)이 주어졌을 때 각 단어가 출현할 확률을 나타낸다
# 이 벡터를 위의 loss 함수에 적용한다면 label 데이터는 정답 단어를 제외한 나머지 단어는 모두 0이기 때문에 결국 정답 단어에 대해 해당 모델이 어떤 점수를 산출했느냐의 관계만을 산출하게 된다.
# 즉, P(w)가 w단어가 나올 확률일 때 Loss 함수를 다음과 같이 나타낼 수 있다.
# L = -logP(w.t | w.t-1,w.t+1) (window size == 1)
# L = (-1/T) * (t=1~T).Σ log(P(w.t | w.t-1,w.t+1)) # 전체 데이터에 대한 loss

### Skip-gram 모델
# CBOW에서 다루는 맥락과 타깃을 역전시킨 모델
# 중앙 단어(타깃)으로 부터 주변 단어(맥락)을 추측하는 모델
# 입력층은 하나의 단어의 원핫 벡터이고 출력층은 맥락의 수만큼 존재하기 때문에 각 맥락과 정답 레이블을 비교해 손실을 개별 손실들을 모두 더한 값을 최종 손실로 한다.
# 정답 레이블은 맥락 단어마다 다르게 설정된다. 입력 word 주변에 say와 car라는 단어가 맥락으로 존재해야 상황이라고 가정했을 때 skip-gram 모델은 추론과정을 거쳐 2개의 output을 산출하게 되고
# 각 output에는 정답 레이블(say,car)이 각각 존재한다. output벡터(확률)과 정답 레이블을 비교해 손실을 각각 산출하고 개별 산출을 모두 더하여 최종 손실을 구한다.
# Skip-gram의 모델을 다음과 같다. 
# P(w.t-1,w.t+1 | w.t) (w.t가 주어졌을 때 w.t-1와 w.t+1 이 동시에 발생할 확률)
# 여기서 skip-gram 모델은 맥락 단어 사이의 관련성이 없다고 가정(조건부 독립), 다음과 같이 분해한다.
# P(w.t-1,w.t+1 | w.t) = P(w.t-1 | w.t) * P(w.t+1|w.t)
# 이 모델에 cross entrophy 모델을 적용하면 다음과 같다.
# L = -log(P(w.t-1,w.t+1 | w.t))
#   = -log(P(w.t-1|w.t)*P(w.t+1|w.t))
#   = -(log(P(w.t-1|w.t)) + log(P(w.t+1|w.t))) (logxy = logx + logy)
# 따라서 전체 데이터에 대한 loss 함수는 다음과 같다.
# L = -(1/T) * (t=1~.T).Σ(logP(w.t-1|w.t)+logP(w.t+1|w.t))


# CBOW는 타깃 하나의 손실만을 구하지만 Skip-gram은 맥락 수 만큼의 손실을 구한다!
# 단어의 분산 표현의 정밀도, 저빈도 단어나 유추 문제에서 skip-gram의 성능이 더 뛰어나다(일반적으로 skip-gram이 더 좋습니다.)
# 학습 속도 면에서는 CBOW가 더 빠릅니다.(skip-gram은 모든 맥락의 손실값을 구해야함..)


### 통계 기반 vs 추론 기반
# 통계 기반 기법은 한 번에 학습, 추론 기반 기법은 미니배치 단위로 나눠서 학습
# 1. 어휘에 추가할 새 단어가 생겨서 분산 표현을 갱신해야 하는 경우
# SVD : 새로운 동시발생 행렬 다시 만들어야함....
# 추론기반 : 지금까지 학습한 가중치를 초기값으로 사용하여 다시 학습하면 됨.(학습의 경험을 해치지 않으며 단어의 분산 표현을 효율적으로 갱신)
# 2. 단어의 분산 표현과 정밀도 측면
# SVD : 주로 단어의 유사성이 인코딩 됨.
# 추론기반 : 단어의 유사성은 물론 단어 사이의 패턴까지 파악(king - man + woman = queen과 같은 유추문제도 풀 수 있다!)

# 학습의 효율 측면에서는 확실히 추론기반이 좋지만 실제 단어 유사성을 정량 평가했을 때 의외로 우열을 가릴 수 없다고 한다.
# 두 세계는 특정 조건하에 연결되어 있다!(추론기법에서 통계기반의 방법이 적용되는 경우가 존재한다)

# GloVe 기법 : 추론 기반 기법과 통계 기반 기법을 융합한 기법
# 말뭉치 전체의 통계정보를 손실 함수에 도입해 미니배치 학습 진행.