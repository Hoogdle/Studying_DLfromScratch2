### 기울기 소실과 LSTM ###

# ### LSTM의 인터페이스
# LSTM은 RNN과 달리 'c'라는 기억 셀이 존재한다.
# 기억 셀의 특징은 데이터를 자기 자신만으로만(LSTM 계층 내에서만) 주고 받는다는 것이다.
# 기억셀의 데이터는 LSTM 계층 내에서만 완결되며, 출력으로 흘러가지 않는다.
# 그와 반대로 LSTM의 은닉상태 'h'는 출력으로도 흘러간다.


### LSTM 계층 조립하기
# c.t는 c.t-1와 x.t, h.t-1의 '어떤 연산'(이 연산은 추후 자세히 설명)으로 만들어지며 h.t는 c.t에다가 just tanh를 입힌 것 뿐이다.
# 즉, h.t = tanh(c.t)

# c.t와 h.t의 원소 수는 같다.(ex) c.t의 원수 수가 100개, h.t의 원소 수도 100개)

# '게이트'란 물(데이터)의 흐름을 얼마나 제어할 것인지 결정하는 것으로 열고 닫는것 뿐만 아니라, 얼마나 흐르게할 것인지 조절할 수 있다.(ex, 70%,80%)
# "게이트를 얼마나 열것인지?"도 학습이 된다.


### output 게이트
# output게이트는 tanh(c.t) 즉 h.t가 다음 시각의 은닉 상태에 얼마나 중요한가를 조정한다.
# h.t의 출력을 담당하는 게이트이므로 output게이트라고 한다.

# output 게이트의 열림 상태는 다음과 같이 구한다
# o = σ(x.t*Wx.o + h.t-1*Wh.o + b.o) (o는 output게이트를 의미하는 것으로 o가 붙은 W는 output게이트의 가중치를 의미한다. σ는 시그모이드를 의미한다.)
# 위 식의 출력값 o와 tanh(c.t)의 모든 원소를 곱하여 h.t를 구하며 이 h.t가 다음 계층으로 넘어가며 이번 계층의 출력이 된다.
# h.t = o ⊙ tanh(c.t) (⊙는 원소별 곱셈을 의미한다)

# cf)
# tanh의 출력값은 -1~1 사이의 값 이며 이 수치안에 데이터의 인코딩된 '정보'의 강약(정도)를 표시한다.
# sigmoid의 출력값은 0~1 사이의 값 이며 이 수치안에 데이터를 얼마만큼 통과시킬지를 정하는 비율이 표시된다.
# 실질적인 정보를 지니는 데이터에는 tanh를, 게이트에서는 시그모이드가 주로 사용된다.


### forget 게이트
# forget 게이트는 c.t-1 중에서 불필요한 기억을 잊게 만들어준다.
# f = σ(x.t*Wx.f + h.t-1*Wh.f + b.f) (f는 forget 게이트를 의미하는 것으로 f가 붙은 W는 forget게이트의 가중치를 의미한다.)
# f와 이전 기억셀의 c.t-1와 원소별 곱으로 현재 기억셀 c.t 구하게 된다.
# c.t = f ⊙ c.t-1


### 새로운 기억 셀
# forget을 통해서 잊어야 할 기억은 잊게 되었으나 새로운 기억을 기억하는 기능은 아직 없다.
# 새로 기억해야 할 정보를 기억 셀에 추가해야 한다.
# 이번 셀은 '게이트' 즉 정보의 량을 조절하는 것이 아닌 새로운 정보를 기억 셀에 추가하는 것을 목적으로 한다.
# 따라서 시그모이드가 아닌 tanh 함수를 사용한다.
# g = tanh(x.t*Wx.g + h.t-1*Wh.g + b.g) (g는 기억셀에 추가하는 새로운 기억)
# c.t-1와 g가 더해짐으로써 새로운 기억 c.t가 생겨난다.


### input 게이트
# g에 게이트를 추가한 것으로 새로운 정보 중 적절한 정보를 취사선택 하여 새로운 기억셀에 정보를 넘기는 게이트이다.
# i = σ(x.t*Wx.i + h.t-1*Wh.i + b.i)



### LSTM의 기울기 흐름
# 그렇다면 LSTM은 어떻게 기울기의 소실을 막을 수 있는 것일까?
# 역전파 과정을 보면 c 경로에는 '+','x' 연산만이 존재하고 x 연산은 '원소별 곱'의 연산이기 때문에 역전파에서의 기울기 소실은 발생하지 않는다.