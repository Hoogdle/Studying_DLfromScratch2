### seq2seq ###

# 시계열 데이터를 다른 시계열 데이터로 변환하는 모델
# 2개의 RNN을 사용하는 seq2seq

### seq2seq의 원리
# Encoder-Decoder 모델
# Encoder는 입력 데이터를 인코딩(부호화), Decoder는 인코딩된 데이터를 디코딩(복호화)
# 부호화 : 'A' => 1000001 # 정보를 규칙에 따라 변환
# 복호화 : 1000001 => 'A' # 인코딩된 정보를 원래의 정보로 되돌림

# "나는 고양이로소이다" => Encoder => Decoder => "I am a cat"
# Encoder가 인코딩한 정보에는 번역에 필요한 정보가 조밀하게 응축되어 있음
# Decoder는 조밀하게 응축된 이 정보를 바탕으로 도착어 문장을 생성

# LSTM의 각 계층을 거쳐 마지막 은닉 상태 h에 입력 문장을 번역하는데 필요한 정보가 인코딩 된다.
# h는 고정 길이 벡터!
# 즉 인코딩은 임의의 길이 문장을 고정 길이 벡터로 변환하는 작업이다.

# Decoder 부분에서는 첫 번째 LSTM 계층이 고정 길이 벡터로 변환된 h를 받아 문장을 하나하나 연속적으로 생성해 내기 시작한다.

# seq2seq의 Encoder와 Decoder는 '가교'로 연결되어있다.
# 순전파시에는 Encoder에서 인코딩된 정보가 Decoder에게 전달되고 역전파시에는 '가교'를 통해 기울기가 Decoder에서 Encoder로 전해진다.



