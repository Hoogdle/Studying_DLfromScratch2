### 어텐션 응용 ### 

# 어텐션의 중요성과 가능성에 관한 3가지 연구 소개

### 구글 신경망 기계 번역(GNMT)
# 규칙 기반 번역 => 용례 기반 번역 => 확률 기반 번역 => 신경망 기계 번역(NMT)

### 트랜스포머
# RNN 구조는 시계열, 즉 시간 방향으로 순서대로 계산을 하기 때문에 병렬 계산은 기본적으로 불가능하다.
# 딥러닝 학습은 GPU를 활용한 병렬 연산에서 이뤄지므로 큰 문제....
# => RNN 사용을 피하고 싶다!! ==> 트랜스포머

# 트랜스포머는 어텐션으로 구성되어 있으며 '셀프어텐션' 기술을 이용한다.(자신에 대한 주목)
# '하나의 시계열 데이터'("나는 고양이로소이다")를 대상으로 각 원소('나','는','고양이','로소이다')가 다른 원소들('나','는','고양이','로소이다')과 어떻게 관련되는지 살펴보는 취지 
# 우리가 지금 까지 구현한 attention 계층의 입력으로는 Encoder의 은닉상태 행렬과 Decoder LSTM의 은닉상태 벡터가 들어 갔지만 
# self attention 에서는 두 입력선이 모두 하나의 시계열 데이터로부터 온다. 
# => 계산량을 줄이며 GPU를 이용한 병렬 계산의 혜택을 누릴 수 있다.
# => 학습 시간을 큰 폭으로 줄이며 번역 품질 또한 상당 폭으로 끌어 올림 => 계산량과 정확도 두 측면에서 모두 유망한 기술


### 뉴럴 튜링 머신(NTM)
# 인간이 복잡한 문제를 풀 때 '종이'와 '펜'(외부의 저장장치)를 사용하듯 외부 메모리를 사용하여 Encoder가 필요한 정보를 메모리에 쓰고
# Decoder는 그 메모리로부터 필요한 정보를 읽어 들일 수 있다.
# 즉 RNN의 외부에 정보 저장용 메모리 기능을 배치하고 어텐션을 이용하여 그 메모리로부터 필요한 정보를 읽거나 쓰는 기술(NTM)
